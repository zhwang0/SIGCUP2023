{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SIGCUP 23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ynvfCyI3AK-c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import rasterio\n",
        "from rasterio.features import shapes as RSHAPES\n",
        "import cv2\n",
        "import geopandas as gpd\n",
        "import shapely\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numpy version =  1.23.3\n",
            "pandas version =  1.4.4\n",
            "rasterio version =  1.3.8\n",
            "cv2 version =  4.6.0\n",
            "geopandas version =  0.14.0\n",
            "shapely version =  2.0.1\n",
            "tensorflow version =  2.10.0\n"
          ]
        }
      ],
      "source": [
        "print('numpy version = ', np.__version__)\n",
        "print('pandas version = ', pd.__version__)\n",
        "print('rasterio version = ', rasterio.__version__)\n",
        "print('cv2 version = ', cv2.__version__)\n",
        "print('geopandas version = ', gpd.__version__)\n",
        "print('shapely version = ', shapely.__version__)\n",
        "print('tensorflow version = ', tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['TrainData\\\\Processed\\\\2019-06-03_roi0.tif', 'TrainData\\\\Processed\\\\2019-06-03_roi1.tif', 'TrainData\\\\Processed\\\\2019-06-03_roi2.tif', 'TrainData\\\\Processed\\\\2019-06-03_roi3.tif', 'TrainData\\\\Processed\\\\2019-06-03_roi4.tif', 'TrainData\\\\Processed\\\\2019-06-03_roi5.tif', 'TrainData\\\\Processed\\\\2019-06-19_roi0.tif', 'TrainData\\\\Processed\\\\2019-06-19_roi1.tif', 'TrainData\\\\Processed\\\\2019-06-19_roi2.tif', 'TrainData\\\\Processed\\\\2019-06-19_roi3.tif', 'TrainData\\\\Processed\\\\2019-06-19_roi4.tif', 'TrainData\\\\Processed\\\\2019-06-19_roi5.tif', 'TrainData\\\\Processed\\\\2019-07-31_roi0.tif', 'TrainData\\\\Processed\\\\2019-07-31_roi1.tif', 'TrainData\\\\Processed\\\\2019-07-31_roi2.tif', 'TrainData\\\\Processed\\\\2019-07-31_roi3.tif', 'TrainData\\\\Processed\\\\2019-07-31_roi4.tif', 'TrainData\\\\Processed\\\\2019-07-31_roi5.tif', 'TrainData\\\\Processed\\\\2019-08-25_roi0.tif', 'TrainData\\\\Processed\\\\2019-08-25_roi1.tif', 'TrainData\\\\Processed\\\\2019-08-25_roi2.tif', 'TrainData\\\\Processed\\\\2019-08-25_roi3.tif', 'TrainData\\\\Processed\\\\2019-08-25_roi4.tif', 'TrainData\\\\Processed\\\\2019-08-25_roi5.tif']\n"
          ]
        }
      ],
      "source": [
        "# ------------- Please modify here ------------- \n",
        "\n",
        "# the directory of images to be evaluated \n",
        "DIR_INPUT_IMG = 'put here the path of image to be evaluted'\n",
        "# DIR_INPUT_IMG = os.path.join('TrainData', 'Processed') \n",
        "\n",
        "ls_img_path = glob(os.path.join(DIR_INPUT_IMG, '*.tif'))\n",
        "print(ls_img_path)\n",
        "if len(ls_img_path) == 0: \n",
        "  print('Do not find the satellite images!')\n",
        "\n",
        "# the output directory of geopackge\n",
        "DIR_OUTPUT = 'GPKG'\n",
        "if not os.path.exists(DIR_OUTPUT): \n",
        "  os.makedirs(DIR_OUTPUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------- Global Variable ------------- \n",
        "# DO NOT CHANGE \n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "\n",
        "INPUT_X_CHANNEL = 3\n",
        "INPUT_Y_CHANNEL = 2\n",
        "OUTPUT_CHANNEL = 2\n",
        "\n",
        "DIR_MODEL = 'model_ckp'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mgQrlZj9UOo"
      },
      "source": [
        "# Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kqYxN9_ZAK6B"
      },
      "outputs": [],
      "source": [
        "# functions \n",
        "def downsampling(filters, size):\n",
        "  res = tf.keras.Sequential()\n",
        "  res.add(tf.keras.layers.Conv2D(filters, size, strides=1, padding='same', use_bias=True))\n",
        "  res.add(tf.keras.layers.BatchNormalization())\n",
        "  res.add(tf.keras.layers.LeakyReLU())\n",
        "  res.add(tf.keras.layers.Conv2D(filters, size, strides=2, padding='same', use_bias=True))\n",
        "  res.add(tf.keras.layers.BatchNormalization())\n",
        "  res.add(tf.keras.layers.LeakyReLU())\n",
        "  return res\n",
        "\n",
        "\n",
        "def upsampling(filters, size):\n",
        "  res = tf.keras.Sequential()\n",
        "  res.add(tf.keras.layers.Conv2DTranspose(filters, size, strides=2,padding='same', use_bias=True))\n",
        "  res.add(tf.keras.layers.BatchNormalization())\n",
        "  res.add(tf.keras.layers.LeakyReLU())\n",
        "  res.add(tf.keras.layers.Conv2D(filters, size, strides=1, padding='same', use_bias=True))\n",
        "  res.add(tf.keras.layers.BatchNormalization())\n",
        "  res.add(tf.keras.layers.LeakyReLU())\n",
        "  return res\n",
        "\n",
        "\n",
        "def unet():\n",
        "  down_stack = [\n",
        "    downsampling(16, 3),  \n",
        "    downsampling(32, 3), \n",
        "    downsampling(64, 3),\n",
        "  ]\n",
        "\n",
        "  up_stack = [\n",
        "    upsampling(32, 3),  \n",
        "    upsampling(16, 3), \n",
        "  ]\n",
        "\n",
        "  x_trans = upsampling(INPUT_X_CHANNEL, 3)\n",
        "  \n",
        "  m_out = tf.keras.Sequential()\n",
        "  m_out.add(tf.keras.layers.Conv2D(32, 1, strides=1, padding='same', activation=tf.nn.relu))\n",
        "  m_out.add(tf.keras.layers.Conv2D(32, 1, strides=1, padding='same', activation=tf.nn.relu))\n",
        "  m_out.add(tf.keras.layers.Conv2D(OUTPUT_CHANNEL, 1, strides=1, padding='same'))\n",
        "\n",
        "\n",
        "  \n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, INPUT_X_CHANNEL])\n",
        "  x = inputs\n",
        "\n",
        "\n",
        "  # UNet Flow\n",
        "  skips = []\n",
        "\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = x_trans(x)\n",
        "\n",
        "  x = tf.keras.layers.Concatenate()([x, inputs])\n",
        "\n",
        "  output = m_out(x)\n",
        "  \n",
        "  return tf.keras.Model(inputs=inputs, outputs=output)\n",
        "\n",
        "\n",
        "def moderator(): \n",
        "  down_stack = [\n",
        "    downsampling(16, 3),  # (bs, 64, 64, 16)\n",
        "    downsampling(32, 3),  # (bs, 32, 32, 32)\n",
        "    downsampling(64, 3),  # (bs, 16, 16, 64)\n",
        "  ]\n",
        "\n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, INPUT_X_CHANNEL]) #RGB\n",
        "  ls_logits = []\n",
        "  for i in range(9):\n",
        "    ls_logits.append(tf.keras.layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, OUTPUT_CHANNEL])) # logits from 9 branches\n",
        "  model_input = [inputs] + ls_logits\n",
        "  \n",
        "  x = inputs\n",
        "  \n",
        "  for down in down_stack: \n",
        "    x = down(x)\n",
        "    \n",
        "  x = tf.reduce_mean(x, [0, 1, 2], keepdims=True) # [batch, 1, 1, 64]\n",
        "  weights = tf.keras.layers.Conv2D(\n",
        "    9, 1, strides=1,\n",
        "    padding='same', \n",
        "    activation='softmax')(x)\n",
        "\n",
        "  output = weights[...,0:1]*ls_logits[0]+weights[...,0:1] * ls_logits[0]\n",
        "  for i in range(1, 9): \n",
        "    output += weights[..., i:i+1] * ls_logits[i]\n",
        "    \n",
        "  return tf.keras.Model(inputs=model_input, outputs=output)\n",
        " \n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ckp from:  Result\\UNet4\\ckp\\best-17\n",
            "Loaded ckp from:  Result\\UNet4_sub1\\ckp\\best-16\n",
            "Loaded ckp from:  Result\\UNet4_sub2\\ckp\\best-13\n",
            "Loaded ckp from:  Result\\UNet4_sub3\\ckp\\best-17\n",
            "Loaded ckp from:  Result\\UNet4_sub4\\ckp\\best-16\n",
            "Loaded ckp from:  Result\\UNet4_sub5\\ckp\\best-16\n",
            "Loaded ckp from:  Result\\UNet4_sub6\\ckp\\best-21\n",
            "Loaded ckp from:  Result\\UNet4_sub7\\ckp\\best-17\n",
            "Loaded ckp from:  Result\\UNet4_sub8\\ckp\\best-16\n"
          ]
        }
      ],
      "source": [
        "# init models\n",
        "ls_pretrain = []\n",
        "ls_pretrain_opmz = []\n",
        "ls_pretrain_ckp = []\n",
        "for i in range(9):\n",
        "  ls_pretrain.append(unet())\n",
        "  ls_pretrain_opmz.append(tf.keras.optimizers.Adam(1e-4))\n",
        "  ls_pretrain_ckp.append(tf.train.Checkpoint(u_optimizer=ls_pretrain_opmz[-1], u_model=ls_pretrain[-1]))\n",
        "  \n",
        "  dir_ckp_pretrained = os.path.join(DIR_MODEL, 'UNet4')\n",
        "  if i > 0:\n",
        "    dir_ckp_pretrained += '_sub'+str(i)\n",
        "  with open(os.path.join(dir_ckp_pretrained, 'ckp', 'best.txt'), 'r') as f:\n",
        "    best_ckp_path = f.readline()  \n",
        "  ls_pretrain_ckp[-1].restore(best_ckp_path)\n",
        "  print('Loaded ckp from: ', best_ckp_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OyLVuOLPGWd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ckp from:  Result\\UNet4_moderator4\\ckp\\best-8\n"
          ]
        }
      ],
      "source": [
        "u_opmz = tf.keras.optimizers.Adam(1e-6)\n",
        "u_moderator = moderator()\n",
        "ckp = tf.train.Checkpoint(u_optimizer=u_opmz, u_model=u_moderator)\n",
        "\n",
        "with open(os.path.join(DIR_MODEL, 'UNet4_moderator4', 'ckp', 'best.txt'), 'r') as f:\n",
        "    best_ckp_path = f.readline()\n",
        "ckp.restore(best_ckp_path)\n",
        "print('Loaded ckp from: ', best_ckp_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsQs5PMC9P_5"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "dir_img = os.path.join('TrainData', 'Processed')\n",
        "ls_img_path = glob(os.path.join(dir_img, '*.tif'))\n",
        "ls_img_path.sort()\n",
        "\n",
        "dir_geo_tif = os.path.join(DIR_OUTPUT, 'geo', 'tif')\n",
        "if not os.path.exists(dir_geo_tif): \n",
        "  os.makedirs(dir_geo_tif)\n",
        "\n",
        "dir_geo_plg = os.path.join(DIR_OUTPUT, 'geo', 'plg')\n",
        "if not os.path.exists(dir_geo_plg): \n",
        "  os.makedirs(dir_geo_plg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predictTiff_moderator(ls_pretrain, tf_model, cur_img_path): \n",
        "\n",
        "  with rasterio.open(cur_img_path) as src:\n",
        "    cur_data = [src.read(i+1) for i in range(3)]\n",
        "    meta = src.meta\n",
        "    \n",
        "  cur_data = np.stack(cur_data, axis=-1)\n",
        "  cur_mask = np.all(cur_data == [0,0,0], axis=-1)\n",
        "  cur_mask = ~cur_mask\n",
        "\n",
        "  img_xsize = cur_data.shape[0]\n",
        "  img_ysize = cur_data.shape[1]\n",
        "\n",
        "\n",
        "  ## ------ predict ------\n",
        "  res_lab = np.zeros((img_xsize, img_ysize), dtype=np.bool_)\n",
        "  step_x = IMG_WIDTH // 2\n",
        "  step_y = IMG_HEIGHT // 2\n",
        "\n",
        "  # fill boundary 1\n",
        "  for x in [0, img_xsize-IMG_HEIGHT]: \n",
        "    for y in range(0, img_ysize-IMG_WIDTH+1, IMG_WIDTH):\n",
        "      x1 = x + IMG_HEIGHT\n",
        "      y1 = y + IMG_WIDTH\n",
        "      \n",
        "      if np.sum(cur_mask[x:x1, y:y1]) == 0: \n",
        "        continue \n",
        "      \n",
        "      cur_x = tf.image.per_image_standardization(cur_data[x:x1, y:y1])\n",
        "      cur_x = tf.expand_dims(cur_x, axis=0)\n",
        "      \n",
        "      ls_logis = []\n",
        "      for cur_pretrain in ls_pretrain: \n",
        "        cur_logits = cur_pretrain(cur_x, training=False)\n",
        "        cur_logits = tf.nn.softmax(cur_logits)\n",
        "        ls_logis.append(cur_logits)\n",
        "      cur_x = [cur_x] + ls_logis\n",
        "        \n",
        "      cur_pred = tf_model(cur_x, training=False)\n",
        "      cur_pred = tf.nn.softmax(cur_pred)\n",
        "      cur_pred = tf.cast(tf.argmax(cur_pred[0], axis=-1), tf.bool).numpy()\n",
        "\n",
        "      res_lab[x:x1, y:y1] = cur_pred\n",
        "\n",
        "  # fill boundary 2\n",
        "  for y in [0, img_ysize-IMG_WIDTH]: \n",
        "    for x in range(0, img_xsize-IMG_HEIGHT+1, IMG_HEIGHT):\n",
        "      x1 = x + IMG_HEIGHT\n",
        "      y1 = y + IMG_WIDTH\n",
        "      \n",
        "      if np.sum(cur_mask[x:x1, y:y1]) == 0: \n",
        "        continue \n",
        "      \n",
        "      cur_x = tf.image.per_image_standardization(cur_data[x:x1, y:y1])\n",
        "      cur_x = tf.expand_dims(cur_x, axis=0)\n",
        "      \n",
        "      ls_logis = []\n",
        "      for cur_pretrain in ls_pretrain: \n",
        "        cur_logits = cur_pretrain(cur_x, training=False)\n",
        "        cur_logits = tf.nn.softmax(cur_logits)\n",
        "        ls_logis.append(cur_logits)\n",
        "      cur_x = [cur_x] + ls_logis\n",
        "      \n",
        "      cur_pred = tf_model(cur_x, training=False)\n",
        "      cur_pred = tf.nn.softmax(cur_pred)\n",
        "      cur_pred = tf.cast(tf.argmax(cur_pred[0], axis=-1), tf.bool).numpy()\n",
        "\n",
        "      res_lab[x:x1, y:y1] = cur_pred\n",
        "\n",
        "  # fill center\n",
        "  x_flag = True\n",
        "  for x in range(0, img_xsize, step_x): \n",
        "    ls_x = []\n",
        "    ls_y_idx = []\n",
        "    y_flag = True \n",
        "    for y in range(0, img_ysize, step_y):\n",
        "      x1 = x + IMG_HEIGHT\n",
        "      y1 = y + IMG_WIDTH\n",
        "      \n",
        "      if x1 > img_xsize:\n",
        "        if x_flag: \n",
        "          x1 = img_xsize\n",
        "          x = x1 - IMG_HEIGHT\n",
        "          x_flag = False\n",
        "        else: \n",
        "          continue\n",
        "      if y1 > img_ysize:\n",
        "        if y_flag: \n",
        "          y1 = img_ysize\n",
        "          y = y1 - IMG_WIDTH\n",
        "          y_flag = False\n",
        "        else: \n",
        "          continue\n",
        "      \n",
        "      if np.sum(cur_mask[x:x1, y:y1]) == 0: \n",
        "        continue \n",
        "      \n",
        "      ls_y_idx.append(y)       \n",
        "      cur_x = tf.image.per_image_standardization(cur_data[x:x1, y:y1])\n",
        "      ls_x.append(tf.expand_dims(cur_x, axis=0))\n",
        "    \n",
        "    \n",
        "    if len(ls_x) > 0: \n",
        "      ls_x = tf.concat(ls_x, axis=0)\n",
        "      ls_x = tf.data.Dataset.from_tensor_slices(ls_x)\n",
        "      ls_x = ls_x.batch(100)\n",
        "      \n",
        "      ls_y = []\n",
        "      for cur_x_ds in ls_x: \n",
        "      \n",
        "        ls_logis = []\n",
        "        for cur_pretrain in ls_pretrain: \n",
        "          cur_logits = cur_pretrain(cur_x_ds, training=False)\n",
        "          cur_logits = tf.nn.softmax(cur_logits)\n",
        "          ls_logis.append(cur_logits)\n",
        "        cur_x_ds = [cur_x_ds] + ls_logis\n",
        "        \n",
        "        cur_pred = tf_model(cur_x_ds, training=False)\n",
        "        cur_pred = tf.nn.softmax(cur_pred)\n",
        "        cur_pred = tf.cast(tf.argmax(cur_pred, axis=-1), tf.bool)\n",
        "        ls_y.append(cur_pred)\n",
        "      ls_y = tf.concat(ls_y, axis=0).numpy()\n",
        "          \n",
        "      for i in range(len(ls_y_idx)):\n",
        "        y = ls_y_idx[i]\n",
        "        y1 = y + IMG_WIDTH\n",
        "        \n",
        "        res_lab[x+int(step_x/2):x1-int(step_x/2), y+int(step_y/2):y1-int(step_y/2)] = ls_y[i][int(step_x/2):int(step_x/2)+step_x, int(step_y/2):int(step_y/2)+step_y]\n",
        "\n",
        "    else: \n",
        "      continue\n",
        "\n",
        "  res_lab = np.logical_and(res_lab, cur_mask)\n",
        "  res_meta = meta.copy()\n",
        "  res_meta.update({\n",
        "    'nodata': None,\n",
        "    'count': 1,\n",
        "    })\n",
        "\n",
        "  return res_lab, res_meta\n",
        "\n",
        "\n",
        "def imgMph(img, kernel_size): \n",
        "  kernel = np.ones((kernel_size, kernel_size),np.uint8)\n",
        "  \n",
        "  res = img.astype(float)\n",
        "  res = cv2.morphologyEx(res, cv2.MORPH_CLOSE, kernel)\n",
        "  res = cv2.morphologyEx(res, cv2.MORPH_OPEN, kernel)\n",
        "  \n",
        "  return res.astype(np.uint8)\n",
        "\n",
        "\n",
        "def img2gdf(np_img, rs_trs, rs_crs, res_name): \n",
        "  res_polygon = RSHAPES(np_img, np_img==1, transform=rs_trs)\n",
        "\n",
        "  ls_geo = []\n",
        "  for geom, _ in res_polygon:\n",
        "    ls_geo.append(shapely.geometry.shape(geom))\n",
        "\n",
        "  gdf = gpd.GeoDataFrame(geometry=ls_geo)\n",
        "  gdf.crs = rs_crs\n",
        "  gdf['Area'] = gdf.area\n",
        "  \n",
        "  gdf_name = generateOfficialName(res_name.split('_roi')[0])\n",
        "  gdf_roi = int(res_name.split('_roi')[1][:1]) + 1\n",
        "  gdf['image'] = gdf_name\n",
        "  gdf['region_num'] = gdf_roi\n",
        "  gdf = gdf[['image', 'region_num', 'geometry', 'Area']] # remove Area column before submission\n",
        "  \n",
        "  return gdf\n",
        "\n",
        "\n",
        "def generateOfficialName(str_date):\n",
        "  NAME_PREFIX = 'Greenland26X_22W_Sentinel2_'\n",
        "  LS_NAME_SUFFIX = ['_05.tif', '_20.tif', '_25.tif', '_29.tif']\n",
        "  \n",
        "  gdf_name = str_date\n",
        "  if gdf_name == '2019-06-03':\n",
        "    gdf_name += LS_NAME_SUFFIX[0]\n",
        "  elif gdf_name == '2019-06-19':\n",
        "    gdf_name += LS_NAME_SUFFIX[1]\n",
        "  elif gdf_name == '2019-07-31':\n",
        "    gdf_name += LS_NAME_SUFFIX[2]\n",
        "  elif gdf_name == '2019-08-25':\n",
        "    gdf_name += LS_NAME_SUFFIX[3]\n",
        "  else: \n",
        "    print('Wrong image date detected!')\n",
        "  gdf_name = NAME_PREFIX + gdf_name\n",
        "  return gdf_name\n",
        "\n",
        "\n",
        "def removeOverlay(gdf):\n",
        "  mask = pd.Series([True] * len(gdf), index=gdf.index)\n",
        "\n",
        "  for i, poly1 in enumerate(gdf.geometry):\n",
        "      for j, poly2 in enumerate(gdf.geometry):\n",
        "          if i != j and poly1.overlaps(poly2):\n",
        "              mask[i] = False\n",
        "              break\n",
        "  \n",
        "  out = gdf[mask]\n",
        "  if len(out) != len(gdf): \n",
        "    print(f'Found {len(gdf)-len(out)} overlaid polygons. Removed..')\n",
        "  return out\n",
        "\n",
        "\n",
        "def computeF1_GPD(gdf_true, gdf_pred):\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "\n",
        "  threshold = 0.5\n",
        "  for poly1 in gdf_true.geometry:\n",
        "      match_found = False\n",
        "      for poly2 in gdf_pred.geometry:\n",
        "          overlap_area = poly1.intersection(poly2).area\n",
        "          \n",
        "          if overlap_area / poly1.area > threshold:\n",
        "              match_found = True\n",
        "              tp += 1\n",
        "              break\n",
        "      if not match_found:\n",
        "          fn += 1\n",
        "\n",
        "  for poly2 in gdf_pred.geometry:\n",
        "      if not any(poly1.intersection(poly2).area / poly1.area > threshold for poly1 in gdf_true.geometry):\n",
        "          fp += 1\n",
        "\n",
        "  precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
        "  recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "  \n",
        "  # print(f\"F1 Score: {f1_score:.4f}\")\n",
        "  return f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "ls_img_path = [\n",
        " 'TrainData\\\\Processed\\\\2019-06-03_roi1.tif',\n",
        " 'TrainData\\\\Processed\\\\2019-06-19_roi2.tif']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [03:42<00:00, 111.27s/it]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ls_output = []\n",
        "for cur_img_path in ls_img_path: \n",
        "  cur_name = os.path.basename(cur_img_path)\n",
        "  output_name = os.path.join(dir_geo_tif, 'pred_'+cur_name)\n",
        "\n",
        "\n",
        "  # predic\n",
        "  if not os.path.exists(output_name):\n",
        "    res_lab, res_meta = predictTiff_moderator(ls_pretrain, u_moderator, cur_img_path)\n",
        "    res_lab = res_lab.astype(np.uint8)\n",
        "    with rasterio.open(output_name, \"w\", **res_meta) as dest:\n",
        "      dest.write(res_lab, 1)\n",
        "  else: \n",
        "    with rasterio.open(output_name) as src:\n",
        "      res_lab = src.read(1)\n",
        "      res_meta = src.meta\n",
        "      \n",
        "      \n",
        "  # post processing\n",
        "  # 1. minimal convex hull\n",
        "  contours, _ = cv2.findContours(res_lab, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "  for cnt in contours:\n",
        "    hull = cv2.convexHull(cnt)\n",
        "    cv2.drawContours(res_lab, [hull], 0, (255), 2)\n",
        "  \n",
        "  # 2. image mophological operation  \n",
        "  res_post = imgMph(res_lab, 10)\n",
        "  \n",
        "  # 3. overlay and area filter\n",
        "  res_gdf = img2gdf(res_post, res_meta['transform'], res_meta['crs'], cur_name)\n",
        "  res_gdf = removeOverlay(res_gdf)\n",
        "  res_gdf = res_gdf[res_gdf.area > 100000]\n",
        "  \n",
        "  # 4. stats filter\n",
        "  with rasterio.open(cur_img_path) as src:\n",
        "    cur_data = [src.read(i+1) for i in range(3)]\n",
        "    meta = src.meta\n",
        "  cur_data = np.stack(cur_data, axis=-1)\n",
        "  \n",
        "  ls_flag = []\n",
        "  for i in range(len(res_gdf)):\n",
        "    cur_plg = rasterio.features.rasterize(res_gdf.iloc[i:i+1].geometry,\n",
        "                                          out_shape=(res_meta['height'], res_meta['width']),\n",
        "                                          transform=res_meta['transform'], dtype='uint16')\n",
        "    cur_plg_rgb = cur_data[cur_plg==1]\n",
        "    if np.std(cur_plg_rgb) < 20: \n",
        "      # within all training polygons, std of rgb > 20\n",
        "      cur_flag = False\n",
        "    else: \n",
        "      cur_flag = True\n",
        "    ls_flag.append(cur_flag)\n",
        "  res_gdf = res_gdf.loc[ls_flag]\n",
        "\n",
        "  ls_output.append(res_gdf)\n",
        "\n",
        "ls_output = pd.concat(ls_output, ignore_index=True)\n",
        "ls_output = ls_output[['image', 'region_num', 'geometry']]\n",
        "ls_output.to_file(os.path.join(DIR_OUTPUT, 'lake_poygons_test.gpkg'), driver=\"GPKG\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
